{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6a84ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and Processed Data:\n",
      "                                  product_name                product_id  \\\n",
      "1  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
      "2  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
      "3  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
      "4  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
      "5  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
      "\n",
      "                category_id       category                       _id   status  \\\n",
      "1  638590305a556431bf362c0a  1000 Ma Bazar  636b55797e71f1fa8bd736b6  removed   \n",
      "2  638590305a556431bf362c0a  1000 Ma Bazar  636ccaa7804ab22b48e0af07  removed   \n",
      "3  638590305a556431bf362c0a  1000 Ma Bazar  636cdfeea5c31a2b645f6cc3  removed   \n",
      "4  638590305a556431bf362c0a  1000 Ma Bazar  636f07e31a01845aedc57ebb  removed   \n",
      "5  638590305a556431bf362c0a  1000 Ma Bazar  63710cef43574ca986227018  removed   \n",
      "\n",
      "   is_order  is_payed                                      url_key  \\\n",
      "1     False     False  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "2     False     False  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "3     False     False  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "4     False     False  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "5     False     False  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "\n",
      "                sku_from_system  ...      store_name  \\\n",
      "1  1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
      "2  1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
      "3  1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
      "4  1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
      "5  1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
      "\n",
      "                    cart_id                  added_by  \\\n",
      "1  636b55797e71f1fa8bd736b5  636b4ffd7e71f1fa8bd7356f   \n",
      "2  636ccaa7804ab22b48e0af06  6360e6483435d7cd920cdf24   \n",
      "3  636cdfeea5c31a2b645f6cc2  636cdf34804ab22b48e0b24c   \n",
      "4  636f07d85458785af3fd9a9c  636f05e45458785af3fd9a78   \n",
      "5  63710cef43574ca986227017  6370f52fd73294a9a96d7a05   \n",
      "\n",
      "                customer_id                   user_id  \\\n",
      "1  636b4ffd7e71f1fa8bd7356f  636b4ffd7e71f1fa8bd7356f   \n",
      "2  6360e6483435d7cd920cdf24  6360e6483435d7cd920cdf24   \n",
      "3  636cdf34804ab22b48e0b24c  636cdf34804ab22b48e0b24c   \n",
      "4  636f05e45458785af3fd9a78  636f05e45458785af3fd9a78   \n",
      "5  6370f52fd73294a9a96d7a05  6370f52fd73294a9a96d7a05   \n",
      "\n",
      "                   brand_id           brand  \\\n",
      "1  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
      "2  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
      "3  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
      "4  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
      "5  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
      "\n",
      "                               product_url_key  \\\n",
      "1  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "2  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "3  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "4  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "5  propods-p9-wireless-gaming-headphones-ipx47   \n",
      "\n",
      "                                 product_description rating  \n",
      "1  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.5  \n",
      "2  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.5  \n",
      "3  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.5  \n",
      "4  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.5  \n",
      "5  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5808\\1120859868.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['customer_id'] = df_cleaned['customer_id'].astype(str)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5808\\1120859868.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['product_id'] = df_cleaned['product_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cartItemsWithRating.csv') # If you are loading from a file\n",
    "\n",
    "# 1. Handle missing values\n",
    "# Dropping rows where essential columns like user_id, product_id, or rating are missing\n",
    "df_cleaned = df.dropna(subset=['customer_id', 'product_id', 'rating'])\n",
    "\n",
    "# 2. Convert 'user_id' and 'product_id' to string to avoid issues in indexing\n",
    "df_cleaned['customer_id'] = df_cleaned['customer_id'].astype(str)\n",
    "df_cleaned['product_id'] = df_cleaned['product_id'].astype(str)\n",
    "\n",
    "# 3. Remove any duplicate entries\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['customer_id', 'product_id'], keep='last')\n",
    "\n",
    "# # 4. Create a user-item matrix (pivot table)\n",
    "# user_item_matrix = df_cleaned.pivot(index='customer_id', columns='product_id', values='rating')\n",
    "\n",
    "# # 5. Fill NaN values with 0 (or alternatively, with a neutral rating, if you prefer)\n",
    "# user_item_matrix = user_item_matrix.fillna(0)\n",
    "\n",
    "# # 6. Normalize the data (optional, depends on the algorithm used)\n",
    "# # Here, we'll normalize ratings to center them around the user's mean rating\n",
    "# user_item_matrix_normalized = user_item_matrix.sub(user_item_matrix.mean(axis=1), axis=0)\n",
    "\n",
    "print(\"Cleaned and Processed Data:\")\n",
    "print(df_cleaned.head())\n",
    "# print(\"\\nUser-Item Matrix:\")\n",
    "# print(user_item_matrix.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e118000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category</th>\n",
       "      <th>_id</th>\n",
       "      <th>status</th>\n",
       "      <th>is_order</th>\n",
       "      <th>is_payed</th>\n",
       "      <th>url_key</th>\n",
       "      <th>sku_from_system</th>\n",
       "      <th>...</th>\n",
       "      <th>store_name</th>\n",
       "      <th>cart_id</th>\n",
       "      <th>added_by</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_url_key</th>\n",
       "      <th>product_description</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>636b55797e71f1fa8bd736b6</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>636b55797e71f1fa8bd736b5</td>\n",
       "      <td>636b4ffd7e71f1fa8bd7356f</td>\n",
       "      <td>636b4ffd7e71f1fa8bd7356f</td>\n",
       "      <td>636b4ffd7e71f1fa8bd7356f</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>636ccaa7804ab22b48e0af07</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>636ccaa7804ab22b48e0af06</td>\n",
       "      <td>6360e6483435d7cd920cdf24</td>\n",
       "      <td>6360e6483435d7cd920cdf24</td>\n",
       "      <td>6360e6483435d7cd920cdf24</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>636cdfeea5c31a2b645f6cc3</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>636cdfeea5c31a2b645f6cc2</td>\n",
       "      <td>636cdf34804ab22b48e0b24c</td>\n",
       "      <td>636cdf34804ab22b48e0b24c</td>\n",
       "      <td>636cdf34804ab22b48e0b24c</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>636f07e31a01845aedc57ebb</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>636f07d85458785af3fd9a9c</td>\n",
       "      <td>636f05e45458785af3fd9a78</td>\n",
       "      <td>636f05e45458785af3fd9a78</td>\n",
       "      <td>636f05e45458785af3fd9a78</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>63710cef43574ca986227018</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>63710cef43574ca986227017</td>\n",
       "      <td>6370f52fd73294a9a96d7a05</td>\n",
       "      <td>6370f52fd73294a9a96d7a05</td>\n",
       "      <td>6370f52fd73294a9a96d7a05</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663127</th>\n",
       "      <td>KTM DUKE Cake</td>\n",
       "      <td>5efaea39ffc2a23ef854cb13</td>\n",
       "      <td>5ef9ba85ffc2a23ef854a473</td>\n",
       "      <td>Cakes</td>\n",
       "      <td>6170dedd12b472b9138058a8</td>\n",
       "      <td>delivered</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ktm-duke-cake</td>\n",
       "      <td>304_HC-B-51</td>\n",
       "      <td>...</td>\n",
       "      <td>Oho Cakes</td>\n",
       "      <td>60958c122d00b678274a534b</td>\n",
       "      <td>60958bb52d00b678274a5347</td>\n",
       "      <td>60958bb52d00b678274a5347</td>\n",
       "      <td>5f8fcc156a586f1d51ebf038</td>\n",
       "      <td>5ef9db6effc2a23ef854b121</td>\n",
       "      <td>Oho Cake</td>\n",
       "      <td>ktm-duke-cake</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Weight 2 Pound&lt;/li&gt;\\n\\t&lt;li&gt;&lt;strong...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663128</th>\n",
       "      <td>Barbie Bash Cake</td>\n",
       "      <td>5efad502ffc2a23ef854c288</td>\n",
       "      <td>5ef9ba85ffc2a23ef854a473</td>\n",
       "      <td>Cakes</td>\n",
       "      <td>608e794f27181703b5846eed</td>\n",
       "      <td>delivered</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>barbie-bash-cake</td>\n",
       "      <td>304_HC-B-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Oho Cakes</td>\n",
       "      <td>607bfbcd28e4fd1f54fe4ee2</td>\n",
       "      <td>6076891500dfc2127dd65e12</td>\n",
       "      <td>6076891500dfc2127dd65e12</td>\n",
       "      <td>6076891500dfc2127dd65e12</td>\n",
       "      <td>5ef9db6effc2a23ef854b121</td>\n",
       "      <td>Oho Cake</td>\n",
       "      <td>barbie-bash-cake</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Pound 3 Pound&lt;/li&gt;\\n\\t&lt;li&gt;&lt;strong&gt;...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663129</th>\n",
       "      <td>Jeevan Jiune Kaida</td>\n",
       "      <td>5ef98fabffc2a23ef854950a</td>\n",
       "      <td>5ea9573765382e2f086c4cdb</td>\n",
       "      <td>Books</td>\n",
       "      <td>6097aa9c609139ec388b7a8c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>jeevan-jiune-kaida</td>\n",
       "      <td>305_978-9937-8924-5-2</td>\n",
       "      <td>...</td>\n",
       "      <td>NEPALAYA</td>\n",
       "      <td>60979da2cf6664dd0319687e</td>\n",
       "      <td>60910407d462bf741b5c1569</td>\n",
       "      <td>60910407d462bf741b5c1569</td>\n",
       "      <td>60910407d462bf741b5c1569</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>jeevan-jiune-kaida</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Author: Jeevan Kumar Prasain&lt;/li&gt;\\...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663131</th>\n",
       "      <td>Rookmangud Katawal  (Nepali)</td>\n",
       "      <td>5ef999adffc2a23ef854968b</td>\n",
       "      <td>5ea953fe65382e2f086c4ca1</td>\n",
       "      <td>Books</td>\n",
       "      <td>608bc0090fb8a98c32413fea</td>\n",
       "      <td>delivered</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>rookmangud-katawal-nepali</td>\n",
       "      <td>305_978-9937-8740-4-5</td>\n",
       "      <td>...</td>\n",
       "      <td>NEPALAYA</td>\n",
       "      <td>608ba3bc9c6677876ac95a6a</td>\n",
       "      <td>608b9fa383478982ad4f1856</td>\n",
       "      <td>608b9fa383478982ad4f1856</td>\n",
       "      <td>5ece09f899cd7f11130d7ca8</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>rookmangud-katawal-nepali</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Author : Rookmangud Katawal&lt;/li&gt;\\n...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663132</th>\n",
       "      <td>Liveup Plastic Hand Grip (Pair)</td>\n",
       "      <td>5efa09fdffc2a23ef854b652</td>\n",
       "      <td>5ef9fd5bffc2a23ef854b420</td>\n",
       "      <td>Sports &amp; Outdoor</td>\n",
       "      <td>60fa5995298f3425f17fae4b</td>\n",
       "      <td>delivered</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>liveup-plastic-hand-grip-pair</td>\n",
       "      <td>310_LS3101</td>\n",
       "      <td>...</td>\n",
       "      <td>Fitex pvt ltd</td>\n",
       "      <td>60f7ec4481da1258c27a3d72</td>\n",
       "      <td>60f7b768dcd0d6708cc3a72f</td>\n",
       "      <td>60f7b768dcd0d6708cc3a72f</td>\n",
       "      <td>60f7b768dcd0d6708cc3a72f</td>\n",
       "      <td>5ef9fe1effc2a23ef854b423</td>\n",
       "      <td>Liveup</td>\n",
       "      <td>liveup-plastic-hand-grip-pair</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190594 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       product_name                product_id  \\\n",
       "1       Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "2       Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "3       Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "4       Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "5       Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "...                                             ...                       ...   \n",
       "663127                                KTM DUKE Cake  5efaea39ffc2a23ef854cb13   \n",
       "663128                             Barbie Bash Cake  5efad502ffc2a23ef854c288   \n",
       "663129                           Jeevan Jiune Kaida  5ef98fabffc2a23ef854950a   \n",
       "663131                 Rookmangud Katawal  (Nepali)  5ef999adffc2a23ef854968b   \n",
       "663132              Liveup Plastic Hand Grip (Pair)  5efa09fdffc2a23ef854b652   \n",
       "\n",
       "                     category_id          category                       _id  \\\n",
       "1       638590305a556431bf362c0a     1000 Ma Bazar  636b55797e71f1fa8bd736b6   \n",
       "2       638590305a556431bf362c0a     1000 Ma Bazar  636ccaa7804ab22b48e0af07   \n",
       "3       638590305a556431bf362c0a     1000 Ma Bazar  636cdfeea5c31a2b645f6cc3   \n",
       "4       638590305a556431bf362c0a     1000 Ma Bazar  636f07e31a01845aedc57ebb   \n",
       "5       638590305a556431bf362c0a     1000 Ma Bazar  63710cef43574ca986227018   \n",
       "...                          ...               ...                       ...   \n",
       "663127  5ef9ba85ffc2a23ef854a473             Cakes  6170dedd12b472b9138058a8   \n",
       "663128  5ef9ba85ffc2a23ef854a473             Cakes  608e794f27181703b5846eed   \n",
       "663129  5ea9573765382e2f086c4cdb             Books  6097aa9c609139ec388b7a8c   \n",
       "663131  5ea953fe65382e2f086c4ca1             Books  608bc0090fb8a98c32413fea   \n",
       "663132  5ef9fd5bffc2a23ef854b420  Sports & Outdoor  60fa5995298f3425f17fae4b   \n",
       "\n",
       "           status  is_order  is_payed  \\\n",
       "1         removed     False     False   \n",
       "2         removed     False     False   \n",
       "3         removed     False     False   \n",
       "4         removed     False     False   \n",
       "5         removed     False     False   \n",
       "...           ...       ...       ...   \n",
       "663127  delivered      True      True   \n",
       "663128  delivered      True      True   \n",
       "663129  delivered      True      True   \n",
       "663131  delivered      True      True   \n",
       "663132  delivered      True      True   \n",
       "\n",
       "                                            url_key  \\\n",
       "1       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "2       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "3       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "4       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "5       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "...                                             ...   \n",
       "663127                                ktm-duke-cake   \n",
       "663128                             barbie-bash-cake   \n",
       "663129                           jeevan-jiune-kaida   \n",
       "663131                    rookmangud-katawal-nepali   \n",
       "663132                liveup-plastic-hand-grip-pair   \n",
       "\n",
       "                     sku_from_system  ...      store_name  \\\n",
       "1       1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
       "2       1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
       "3       1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
       "4       1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
       "5       1378_105926346_NP-1027832760  ...  Rk Jha Traders   \n",
       "...                              ...  ...             ...   \n",
       "663127                   304_HC-B-51  ...       Oho Cakes   \n",
       "663128                   304_HC-B-31  ...       Oho Cakes   \n",
       "663129         305_978-9937-8924-5-2  ...        NEPALAYA   \n",
       "663131         305_978-9937-8740-4-5  ...        NEPALAYA   \n",
       "663132                    310_LS3101  ...   Fitex pvt ltd   \n",
       "\n",
       "                         cart_id                  added_by  \\\n",
       "1       636b55797e71f1fa8bd736b5  636b4ffd7e71f1fa8bd7356f   \n",
       "2       636ccaa7804ab22b48e0af06  6360e6483435d7cd920cdf24   \n",
       "3       636cdfeea5c31a2b645f6cc2  636cdf34804ab22b48e0b24c   \n",
       "4       636f07d85458785af3fd9a9c  636f05e45458785af3fd9a78   \n",
       "5       63710cef43574ca986227017  6370f52fd73294a9a96d7a05   \n",
       "...                          ...                       ...   \n",
       "663127  60958c122d00b678274a534b  60958bb52d00b678274a5347   \n",
       "663128  607bfbcd28e4fd1f54fe4ee2  6076891500dfc2127dd65e12   \n",
       "663129  60979da2cf6664dd0319687e  60910407d462bf741b5c1569   \n",
       "663131  608ba3bc9c6677876ac95a6a  608b9fa383478982ad4f1856   \n",
       "663132  60f7ec4481da1258c27a3d72  60f7b768dcd0d6708cc3a72f   \n",
       "\n",
       "                     customer_id                   user_id  \\\n",
       "1       636b4ffd7e71f1fa8bd7356f  636b4ffd7e71f1fa8bd7356f   \n",
       "2       6360e6483435d7cd920cdf24  6360e6483435d7cd920cdf24   \n",
       "3       636cdf34804ab22b48e0b24c  636cdf34804ab22b48e0b24c   \n",
       "4       636f05e45458785af3fd9a78  636f05e45458785af3fd9a78   \n",
       "5       6370f52fd73294a9a96d7a05  6370f52fd73294a9a96d7a05   \n",
       "...                          ...                       ...   \n",
       "663127  60958bb52d00b678274a5347  5f8fcc156a586f1d51ebf038   \n",
       "663128  6076891500dfc2127dd65e12  6076891500dfc2127dd65e12   \n",
       "663129  60910407d462bf741b5c1569  60910407d462bf741b5c1569   \n",
       "663131  608b9fa383478982ad4f1856  5ece09f899cd7f11130d7ca8   \n",
       "663132  60f7b768dcd0d6708cc3a72f  60f7b768dcd0d6708cc3a72f   \n",
       "\n",
       "                        brand_id           brand  \\\n",
       "1       5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "2       5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "3       5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "4       5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "5       5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "...                          ...             ...   \n",
       "663127  5ef9db6effc2a23ef854b121        Oho Cake   \n",
       "663128  5ef9db6effc2a23ef854b121        Oho Cake   \n",
       "663129  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "663131  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "663132  5ef9fe1effc2a23ef854b423          Liveup   \n",
       "\n",
       "                                    product_url_key  \\\n",
       "1       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "2       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "3       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "4       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "5       propods-p9-wireless-gaming-headphones-ipx47   \n",
       "...                                             ...   \n",
       "663127                                ktm-duke-cake   \n",
       "663128                             barbie-bash-cake   \n",
       "663129                           jeevan-jiune-kaida   \n",
       "663131                    rookmangud-katawal-nepali   \n",
       "663132                liveup-plastic-hand-grip-pair   \n",
       "\n",
       "                                      product_description rating  \n",
       "1       <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.5  \n",
       "2       <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.5  \n",
       "3       <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.5  \n",
       "4       <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.5  \n",
       "5       <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.0  \n",
       "...                                                   ...    ...  \n",
       "663127  <ul>\\n\\t<li>Weight 2 Pound</li>\\n\\t<li><strong...    1.0  \n",
       "663128  <ul>\\n\\t<li>Pound 3 Pound</li>\\n\\t<li><strong>...    2.0  \n",
       "663129  <ul>\\n\\t<li>Author: Jeevan Kumar Prasain</li>\\...    1.5  \n",
       "663131  <ul>\\n\\t<li>Author : Rookmangud Katawal</li>\\n...    2.0  \n",
       "663132  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.0  \n",
       "\n",
       "[190594 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be983a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Convert the cleaned DataFrame to Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_cleaned[['user_id', 'product_id', 'rating']], reader)\n",
    "\n",
    "# Train-test split\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# User-based collaborative filtering\n",
    "algo_user = KNNBasic(sim_options={'user_based': True})\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo_user.fit(trainset)\n",
    "predictions_user = algo_user.test(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529f26db-0fbb-436b-a448-ed4b310483a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0813\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE\n",
    "rmse_user = accuracy.rmse(predictions_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8afa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for user 5e817efd5d1501034c7e7647: ['5ffec57c6d380c0862375b42', '60d330513d2c4e154e5c7dd3', '5ffe98ff6d380c08623758fd', '60a1089583ea5bb6d42f75d1', '6214998e9f391954ea5a0e91', '5e92d5b7206e4c243b0f5c23', '627753d1d28bde77ec6857cd', '60f51545c03365e03673ad26', '62bea74ab04be713f3efbc25', '60e2d6a0b23abf5d0863e8dd']\n"
     ]
    }
   ],
   "source": [
    "# Example: Get top-N recommendations for a specific user (e.g., user_id='101')\n",
    "user_id = '5e817efd5d1501034c7e7647'\n",
    "user_inner_id = algo_user.trainset.to_inner_uid(user_id)\n",
    "user_neighbors = algo_user.get_neighbors(user_inner_id, k=10)\n",
    "\n",
    "# Convert inner IDs to raw IDs for the recommendations\n",
    "user_recommendations = [algo_user.trainset.to_raw_iid(inner_id) for inner_id in user_neighbors]\n",
    "print(f\"Top recommendations for user {user_id}: {user_recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba3f5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.1247\n",
      "Top item-based recommendations for user 5e817efd5d1501034c7e7647: ['62b2c85ae0d735573aab0ab9', '605d8c9e05fab268763998e4', '5facecd47721ea307a62502f', '61ea7b896f45c07e3b001ddd', '62a06c92a473abe75617bd4b', '5f4d35fb6a07b964402bba7f', '601fba44ed2f557ec86a88b5', '5eff07c9effdd92d6d1539bd', '6214892bcf8fc854c7362e61', '606ea9f43d6250467444e4f6']\n"
     ]
    }
   ],
   "source": [
    "# Item-based collaborative filtering\n",
    "algo_item = KNNBasic(sim_options={'user_based': False})\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo_item.fit(trainset)\n",
    "predictions_item = algo_item.test(testset)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_item = accuracy.rmse(predictions_item)\n",
    "\n",
    "# Example: Get top-N recommendations for a specific user (e.g., user_id='101')\n",
    "user_inner_id_item = algo_item.trainset.to_inner_uid(user_id)\n",
    "item_neighbors = algo_item.get_neighbors(user_inner_id_item, k=10)\n",
    "\n",
    "# Convert inner IDs to raw IDs for the recommendations\n",
    "item_recommendations = [algo_item.trainset.to_raw_iid(inner_id) for inner_id in item_neighbors]\n",
    "print(f\"Top item-based recommendations for user {user_id}: {item_recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69678fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e290dc",
   "metadata": {},
   "source": [
    "### Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca021f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'memory_profiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmemory_profiler\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'memory_profiler'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from joblib import Parallel, delayed\n",
    "import memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content = df[['product_id', 'product_name', 'category', 'brand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "# Assuming your dataframe is named df\n",
    "\n",
    "# Selecting relevant columns\n",
    "df_content = df[['product_id', 'product_name', 'category', 'brand']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746811f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Text Preprocessing\n",
    "# # Clean and preprocess the description column\n",
    "# def preprocess_text(text):\n",
    "#     # Here you can add more preprocessing steps like stopwords removal, lemmatization, etc.\n",
    "#     return text.lower()\n",
    "\n",
    "# df_content['cleaned_description'] = df_content['product_description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b03fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "# One-hot encode categories and brands\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the 'category' and 'brand' columns separately\n",
    "category_onehot = onehot_encoder.fit_transform(df_content[['category']])\n",
    "brand_onehot = onehot_encoder.fit_transform(df_content[['brand']])\n",
    "\n",
    "# Combine all features into a single matrix\n",
    "combined_features = np.hstack((category_onehot, brand_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314e38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Chunk-Based Processing Function\n",
    "\n",
    "def process_chunk(start_idx, end_idx, combined_features):\n",
    "    # Compute cosine similarity for a chunk\n",
    "    chunk = combined_features[start_idx:end_idx]\n",
    "    return cosine_similarity(chunk, combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af1f12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 247. GiB for an array with shape (50000, 663133) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cosine_sim\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Compute the cosine similarity matrix using chunks\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m compute_cosine_similarity_in_chunks(combined_features)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mcompute_cosine_similarity_in_chunks\u001b[1;34m(combined_features, chunk_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     start_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m chunk_size\n\u001b[0;32m      9\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m chunk_size, \u001b[38;5;28mlen\u001b[39m(combined_features))\n\u001b[1;32m---> 10\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(process_chunk(start_idx, end_idx, combined_features))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Combine results into a single similarity matrix\u001b[39;00m\n\u001b[0;32m     13\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(results)\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mprocess_chunk\u001b[1;34m(start_idx, end_idx, combined_features)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_chunk\u001b[39m(start_idx, end_idx, combined_features):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Compute cosine similarity for a chunk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m combined_features[start_idx:end_idx]\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity(chunk, combined_features)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1687\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1687\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:205\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    203\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    208\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 247. GiB for an array with shape (50000, 663133) and data type float64"
     ]
    }
   ],
   "source": [
    "# Step 4: Memory Optimization\n",
    "def compute_cosine_similarity_in_chunks(combined_features, chunk_size=50000):\n",
    "    num_chunks = int(np.ceil(len(combined_features) / chunk_size))\n",
    "    \n",
    "    # Determine optimal chunk size based on memory constraints\n",
    "    results = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(combined_features))\n",
    "        results.append(process_chunk(start_idx, end_idx, combined_features))\n",
    "    \n",
    "    # Combine results into a single similarity matrix\n",
    "    cosine_sim = np.vstack(results)\n",
    "    return cosine_sim\n",
    "\n",
    "# Compute the cosine similarity matrix using chunks\n",
    "cosine_sim = compute_cosine_similarity_in_chunks(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess features for TF-IDF\n",
    "def preprocess_features(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['product_name'])\n",
    "    return tfidf_matrix\n",
    "\n",
    "combined_features = preprocess_features(df_content)\n",
    "\n",
    "# Function to process a chunk and save results\n",
    "def process_chunk(start_idx, end_idx, combined_features, output_dir, chunk_index):\n",
    "    chunk = combined_features[start_idx:end_idx]\n",
    "    cosine_sim_chunk = cosine_similarity(chunk, combined_features)\n",
    "    np.save(os.path.join(output_dir, f\"cosine_sim_chunk_{chunk_index}.npy\"), cosine_sim_chunk)\n",
    "    del chunk, cosine_sim_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Function to process chunks in parallel\n",
    "def process_in_chunks(combined_features, output_dir, chunk_size):\n",
    "    num_chunks = int(np.ceil(combined_features.shape[0] / chunk_size))\n",
    "    print(f\"Processing {num_chunks} chunks with chunk size {chunk_size}\")\n",
    "    Parallel(n_jobs=-1)(delayed(process_chunk)(\n",
    "        i * chunk_size, \n",
    "        min((i + 1) * chunk_size, combined_features.shape[0]), \n",
    "        combined_features, \n",
    "        output_dir, \n",
    "        i\n",
    "    ) for i in range(num_chunks))\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = 'output_directory'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "chunk_size = 1000  # Adjust chunk size as needed\n",
    "process_in_chunks(combined_features, output_dir, chunk_size)\n",
    "\n",
    "# Combine all saved chunks into a single matrix\n",
    "def combine_chunks(output_dir, num_chunks, combined_features_shape):\n",
    "    combined_sim = np.zeros(combined_features_shape)\n",
    "    for i in range(num_chunks):\n",
    "        chunk_file = os.path.join(output_dir, f\"cosine_sim_chunk_{i}.npy\")\n",
    "        if os.path.exists(chunk_file):\n",
    "            chunk_sim = np.load(chunk_file)\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, combined_features_shape[0])\n",
    "            combined_sim[start_idx:end_idx, :] = chunk_sim[:end_idx - start_idx, :]\n",
    "    return combined_sim\n",
    "\n",
    "# Combine all chunks\n",
    "num_chunks = int(np.ceil(combined_features.shape[0] / chunk_size))\n",
    "combined_features_shape = (combined_features.shape[0], combined_features.shape[0])\n",
    "cosine_sim = combine_chunks(output_dir, num_chunks, combined_features_shape)\n",
    "\n",
    "print(\"Cosine similarity matrix computation completed.\")\n",
    "\n",
    "# Function to get recommendations\n",
    "def get_recommendations(product_id, cosine_sim, df):\n",
    "    idx = df.index[df['product_id'] == product_id].tolist()\n",
    "    if not idx:\n",
    "        return []\n",
    "    idx = idx[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]  # Get top 10 similar items, excluding the item itself\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    return df.iloc[product_indices]\n",
    "\n",
    "# Example: Get recommendations for a specific product\n",
    "product_id = '1'\n",
    "recommendations = get_recommendations(product_id, cosine_sim, df_content)\n",
    "print(f\"Recommendations for product {product_id}:\\n\", recommendations)\n",
    "\n",
    "# Memory profiling example\n",
    "print(f\"Memory usage: {memory_profiler.memory_usage()} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49554085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "# One-hot encode categories and brands\n",
    "onehot_encoder = OneHotEncoder()\n",
    "category_onehot = onehot_encoder.fit_transform(df_content[['category']])\n",
    "brand_onehot = onehot_encoder.fit_transform(df_content[['brand']])\n",
    "\n",
    "# Combine all features into a single matrix\n",
    "combined_features = np.hstack((category_onehot.toarray(), brand_onehot.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50355e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Cosine Similarity\n",
    "# Compute cosine similarity matrix using sparse matrix\n",
    "# cosine_sim = cosine_similarity(combined_features, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "460e3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import vstack\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3910f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix computation completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import vstack, csr_matrix\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Sample data initialization\n",
    "# Replace this with your actual data preprocessing steps\n",
    "def preprocess_features(df):\n",
    "    # Example feature extraction: TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['product_name'])\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Initialize combined_features\n",
    "df = pd.DataFrame({\n",
    "    'product_name': ['Product A', 'Product B', 'Product C']  # Example product names\n",
    "})\n",
    "\n",
    "combined_features = preprocess_features(df)\n",
    "\n",
    "# Function to process a chunk and save results\n",
    "def process_and_save_chunk(start_idx, end_idx, combined_features, output_dir, chunk_index):\n",
    "    # Extract chunk\n",
    "    chunk = combined_features[start_idx:end_idx]\n",
    "    \n",
    "    # Compute cosine similarity for the chunk\n",
    "    cosine_sim_chunk = cosine_similarity(chunk, combined_features)\n",
    "    \n",
    "    # Save the result\n",
    "    np.save(os.path.join(output_dir, f\"cosine_sim_chunk_{chunk_index}.npy\"), cosine_sim_chunk)\n",
    "    \n",
    "    # Free up memory\n",
    "    del chunk, cosine_sim_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Function to process chunks in parallel\n",
    "def process_in_chunks(combined_features, output_dir, chunk_size):\n",
    "    num_chunks = int(np.ceil(combined_features.shape[0] / chunk_size))\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, combined_features.shape[0])\n",
    "        process_and_save_chunk(start_idx, end_idx, combined_features, output_dir, i)\n",
    "\n",
    "# Example usage:\n",
    "output_dir = 'output_directory'\n",
    "chunk_size = 1000  # Adjust chunk size as needed\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process and save each chunk\n",
    "process_in_chunks(combined_features, output_dir, chunk_size)\n",
    "\n",
    "# Combine all saved chunks into a single matrix\n",
    "def combine_chunks(output_dir, num_chunks, combined_features_shape):\n",
    "    combined_sim = np.zeros(combined_features_shape)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk_file = os.path.join(output_dir, f\"cosine_sim_chunk_{i}.npy\")\n",
    "        if os.path.exists(chunk_file):\n",
    "            chunk_sim = np.load(chunk_file)\n",
    "            combined_sim[i * chunk_size : (i + 1) * chunk_size] = chunk_sim\n",
    "    \n",
    "    return combined_sim\n",
    "\n",
    "# Combine all chunks\n",
    "num_chunks = int(np.ceil(combined_features.shape[0] / chunk_size))\n",
    "combined_features_shape = (combined_features.shape[0], combined_features.shape[0])  # Shape of the full similarity matrix\n",
    "cosine_sim = combine_chunks(output_dir, num_chunks, combined_features_shape)\n",
    "\n",
    "print(\"Cosine similarity matrix computation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84b16893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal chunk size: 0\n"
     ]
    }
   ],
   "source": [
    "min_chunk_size = 0\n",
    "max_chunk_size = 5000\n",
    "current_chunk_size = (min_chunk_size + max_chunk_size) // 2\n",
    "\n",
    "while min_chunk_size < max_chunk_size:\n",
    "    try:\n",
    "        mem_usage = compute_chunk_memory(combined_features[:current_chunk_size])\n",
    "        print(f\"Testing chunk size {current_chunk_size}: Memory usage {mem_usage[0]} MB\")\n",
    "        \n",
    "        if mem_usage[0] < threshold_memory_usage:\n",
    "            min_chunk_size = current_chunk_size + 1\n",
    "        else:\n",
    "            max_chunk_size = current_chunk_size - 1\n",
    "        \n",
    "        current_chunk_size = (min_chunk_size + max_chunk_size) // 2\n",
    "    except MemoryError:\n",
    "        max_chunk_size = current_chunk_size - 1\n",
    "        current_chunk_size = (min_chunk_size + max_chunk_size) // 2\n",
    "\n",
    "print(f\"Optimal chunk size: {current_chunk_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4cefeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_in_chunks(data, chunk_size=500):\n",
    "    num_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "        chunk = data[start_idx:end_idx]\n",
    "        \n",
    "        # Compute cosine similarity between the chunk and the entire dataset\n",
    "        chunk_cosine_sim = cosine_similarity(chunk, data)\n",
    "        \n",
    "        # Convert to sparse matrix and yield the result to save memory\n",
    "        yield csr_matrix(chunk_cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e24bdd9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.43 GiB for an array with shape (663133, 1706) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cosine_sim_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process each chunk and collect results\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_sim \u001b[38;5;129;01min\u001b[39;00m get_similarity_in_chunks(combined_features):\n\u001b[0;32m      6\u001b[0m     cosine_sim_results\u001b[38;5;241m.\u001b[39mappend(chunk_sim)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Optionally, combine all sparse matrices into a single sparse matrix\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# This can be skipped if you just want to process each chunk individually\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m, in \u001b[0;36mget_similarity_in_chunks\u001b[1;34m(data, chunk_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m chunk \u001b[38;5;241m=\u001b[39m data[start_idx:end_idx]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity between the chunk and the entire dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m chunk_cosine_sim \u001b[38;5;241m=\u001b[39m cosine_similarity(chunk, data)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert to sparse matrix and yield the result to save memory\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m csr_matrix(chunk_cosine_sim)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1685\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1683\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m X_normalized\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1685\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1687\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1933\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     sparse_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1931\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m-> 1933\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1934\u001b[0m     X,\n\u001b[0;32m   1935\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39msparse_format,\n\u001b[0;32m   1936\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1937\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe normalize function\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1938\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[0;32m   1939\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1940\u001b[0m )\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1942\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1075\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m-> 1075\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m   1076\u001b[0m             array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m   1077\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m   1081\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m   1082\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:749\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.43 GiB for an array with shape (663133, 1706) and data type float64"
     ]
    }
   ],
   "source": [
    "# Initialize an empty sparse matrix or a list to store results\n",
    "cosine_sim_results = []\n",
    "\n",
    "# Process each chunk and collect results\n",
    "for chunk_sim in get_similarity_in_chunks(combined_features):\n",
    "    cosine_sim_results.append(chunk_sim)\n",
    "\n",
    "# Optionally, combine all sparse matrices into a single sparse matrix\n",
    "# This can be skipped if you just want to process each chunk individually\n",
    "final_cosine_sim = vstack(cosine_sim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Recommendation Functions\n",
    "\n",
    "# General recommendation based on category and brand\n",
    "def get_recommendations(product_id, cosine_sim=cosine_sim, df=df_content):\n",
    "    # Find the index of the product that matches the product_id\n",
    "    idx = df[df['product_id'] == product_id].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all products with that product\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the product indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar products\n",
    "    return df['product_name'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e705b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-based recommendation\n",
    "def get_category_recommendations(product_id, df=df_content):\n",
    "    # Find the category of the given product_id\n",
    "    product_category = df[df['product_id'] == product_id]['category'].values[0]\n",
    "    \n",
    "    # Filter products that belong to the same category\n",
    "    category_products = df[df['category'] == product_category]\n",
    "    \n",
    "    # Exclude the input product itself\n",
    "    category_products = category_products[category_products['product_id'] != product_id]\n",
    "    \n",
    "    # Return the names of the products in the same category\n",
    "    return category_products['product_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206aea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand-based recommendation\n",
    "def get_brand_recommendations(product_id, df=df_content):\n",
    "    # Find the brand of the given product_id\n",
    "    product_brand = df[df['product_id'] == product_id]['brand'].values[0]\n",
    "    \n",
    "    # Filter products that belong to the same brand\n",
    "    brand_products = df[df['brand'] == product_brand]\n",
    "    \n",
    "    # Exclude the input product itself\n",
    "    brand_products = brand_products[brand_products['product_id'] != product_id]\n",
    "    \n",
    "    # Return the names of the products in the same brand\n",
    "    return brand_products['product_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685acbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General recommendations\n",
    "recommended_products = get_recommendations(product_id='5e86177ce463693ae6692294')\n",
    "print(\"General Recommendations:\", recommended_products)\n",
    "\n",
    "# Category-based recommendations\n",
    "category_recommendations = get_category_recommendations(product_id='5e86177ce463693ae6692294')\n",
    "print(\"Category-Based Recommendations:\", category_recommendations)\n",
    "\n",
    "# Brand-based recommendations\n",
    "brand_recommendations = get_brand_recommendations(product_id='5e86177ce463693ae6692294')\n",
    "print(\"Brand-Based Recommendations:\", brand_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a6ac3-5b09-465a-b890-e496a9bc4fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73647b6-fc5f-4b3d-ac61-5ba13461c2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661a72f-ac76-4874-9024-b3216ed208a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb03b57-cfc3-493f-b72c-6b1ff2624f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60f9e496-8558-468f-94b2-88707d34fd33",
   "metadata": {},
   "source": [
    "### Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1949af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "import gc\n",
    "\n",
    "def process_and_index_data(file_path, chunk_size=1000):\n",
    "    chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # Initialize Faiss index later when feature space is known\n",
    "    index = None\n",
    "    num_features = None\n",
    "    \n",
    "    # Store product names and indices to ensure uniqueness\n",
    "    all_product_names = []\n",
    "    all_product_indices = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk['combined_features'] = chunk['product_name'].fillna('') + \" \" + \\\n",
    "                                     chunk['category'].fillna('') + \" \" + \\\n",
    "                                     chunk['brand'].fillna('')\n",
    "        \n",
    "        if num_features is None:\n",
    "            # Fit on the first chunk\n",
    "            chunk_tfidf_matrix = tfidf_vectorizer.fit_transform(chunk['combined_features'])\n",
    "            num_features = chunk_tfidf_matrix.shape[1]\n",
    "            index = faiss.IndexFlatL2(num_features)\n",
    "        else:\n",
    "            chunk_tfidf_matrix = tfidf_vectorizer.transform(chunk['combined_features'])\n",
    "        \n",
    "        # Ensure matrix is float32 for Faiss compatibility\n",
    "        chunk_tfidf_matrix = chunk_tfidf_matrix.astype('float32')\n",
    "        \n",
    "        # Add this chunk to the Faiss index\n",
    "        index.add(chunk_tfidf_matrix.toarray())\n",
    "        \n",
    "        # Keep track of unique product names and their indices\n",
    "        all_product_names.extend(chunk['product_name'].tolist())\n",
    "        all_product_indices.extend(range(len(all_product_names) - len(chunk), len(all_product_names)))\n",
    "        \n",
    "        # Free memory from the chunk and force garbage collection\n",
    "        del chunk_tfidf_matrix, chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Remove duplicates by converting to a DataFrame\n",
    "    product_df = pd.DataFrame({'product_name': all_product_names, 'index': all_product_indices})\n",
    "    product_df.drop_duplicates(subset='product_name', keep='first', inplace=True)\n",
    "    \n",
    "    # Reindex the Faiss index to only include unique products\n",
    "    unique_indices = product_df['index'].values\n",
    "    unique_tfidf_matrix = np.zeros((len(unique_indices), num_features), dtype=np.float32)\n",
    "\n",
    "    for i, idx in enumerate(unique_indices):\n",
    "        unique_tfidf_matrix[i, :] = index.reconstruct(int(idx))\n",
    "    \n",
    "    # Create a new Faiss index with only the unique products\n",
    "    unique_index = faiss.IndexFlatL2(num_features)\n",
    "    unique_index.add(unique_tfidf_matrix)\n",
    "    \n",
    "    return unique_index, tfidf_vectorizer, product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e5fdfe-366d-4c57-a305-5a5f63fe620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_profile(user_id, data, tfidf_vectorizer, chunk_size=5000):\n",
    "    num_features = len(tfidf_vectorizer.get_feature_names_out())\n",
    "    user_profile_vector = np.zeros(num_features, dtype=np.float32)\n",
    "    \n",
    "    user_data = data[data['customer_id'] == user_id]\n",
    "    # print(user_data)\n",
    "    for start in range(0, len(user_data), chunk_size):\n",
    "        end = min(start + chunk_size, len(user_data))\n",
    "        chunk = user_data[start:end]\n",
    "        \n",
    "        chunk_combined_features = chunk['product_name'] + \" \" + chunk['category'] + \" \" + chunk['brand']\n",
    "        chunk_combined_features = chunk_combined_features.fillna('')\n",
    "        \n",
    "        chunk_tfidf_matrix = tfidf_vectorizer.transform(chunk_combined_features)\n",
    "        \n",
    "        user_profile_vector += np.sum(chunk_tfidf_matrix.toarray(), axis=0)\n",
    "    \n",
    "    norm = np.linalg.norm(user_profile_vector)\n",
    "    if norm > 0:\n",
    "        user_profile_vector /= norm\n",
    "    \n",
    "    return user_profile_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d753a02-ce32-4034-9712-cc6a5dedcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_products(product_name, data, tfidf_vectorizer, index, num_recommendations=5):\n",
    "    # Combine the features of the given product\n",
    "    product_data = data[data['product_name'] == product_name].iloc[0]\n",
    "    product_combined_features = (product_data['product_name'] + \" \" + \n",
    "                                 product_data['category'] + \" \" + \n",
    "                                 product_data['brand'])\n",
    "    \n",
    "    # Transform the product into a TF-IDF vector\n",
    "    product_vector = tfidf_vectorizer.transform([product_combined_features]).toarray().astype('float32')\n",
    "    \n",
    "    # Query Faiss to find similar products\n",
    "    _, similar_product_indices = index.search(product_vector, num_recommendations)\n",
    "    \n",
    "    # Return the names of the similar products\n",
    "    similar_products = data['product_name'].iloc[similar_product_indices[0]].tolist()\n",
    "    return similar_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965630cb-de7c-4c92-bd0e-55a86201ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "file_path = 'cartItemsWithRating.csv'\n",
    "chunk_size = 1000\n",
    "\n",
    "# Process data and build Faiss index incrementally\n",
    "index, tfidf_vectorizer, product_df = process_and_index_data(file_path, chunk_size)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379843cf-6ac8-4b21-8765-27a37813d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products(user_id, data, index, tfidf_vectorizer, num_recommendations=5, chunk_size=5000):\n",
    "    user_profile_vector = create_user_profile(user_id, data, tfidf_vectorizer, chunk_size)\n",
    "    \n",
    "    # Increase the number of initial recommendations from Faiss to allow filtering\n",
    "    faiss_recommendations_count = num_recommendations * 2\n",
    "    _, recommended_product_indices = index.search(np.array([user_profile_vector]), faiss_recommendations_count)\n",
    "    \n",
    "    # Filter out duplicate products\n",
    "    seen_products = set()\n",
    "    diverse_recommendations = []\n",
    "    for idx in recommended_product_indices[0]:\n",
    "        # print(data['product_id'].iloc[idx])\n",
    "        product_name = data['product_id'].iloc[idx]\n",
    "        if product_name not in seen_products:\n",
    "            seen_products.add(product_name)\n",
    "            diverse_recommendations.append(product_name)\n",
    "        if len(diverse_recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    return diverse_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ddc1b9e-46be-4634-8aef-7b23af6b6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products recommended for user '63295a1878f932890ebe7f33':\n",
      "622053a4ed5c72013de1aa25\n",
      "61c44130cd966246a2f473f9\n",
      "623d34dc19446be57b6efc26\n",
      "624443ab65c8d010805dce36\n",
      "622da1a4cebfa05f7f24012e\n",
      "608e3e605aadcafede2317d6\n",
      "608e491b935586fe5d8ed7bb\n",
      "608e47f90053e0fe50ae370f\n",
      "61dd1a7956b1cc02904423c0\n",
      "5f1fb0f64d1dbe53b07a933d\n"
     ]
    }
   ],
   "source": [
    "# Example: Recommend products for a specific user\n",
    "user_id_to_recommend = '63295a1878f932890ebe7f33'  # Replace with an actual user_id from your dataset\n",
    "recommended_products = recommend_products(user_id_to_recommend, data, index, tfidf_vectorizer, 10)\n",
    "\n",
    "print(f\"Products recommended for user '{user_id_to_recommend}':\")\n",
    "for product in recommended_products:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91290235-7c29-46a5-8390-dcdc5d8ca262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products similar to 'A Grade Football Jersey - Portugal':\n",
      "Maroon Golden Banarasi Silk Saree For Women (2198507TIJ)\n",
      "FOGG Collection Absolute 15ML\n",
      "Tokla Green Tea Fresh\n",
      "Tokla Green Tea Fresh\n",
      "Complan Royal Chocolate Flavour\n"
     ]
    }
   ],
   "source": [
    "# Example: Get products similar to a specific product\n",
    "product_name_to_recommend = 'A Grade Football Jersey - Portugal'  # Replace with an actual product name from your dataset\n",
    "similar_products = get_similar_products(product_name_to_recommend, data, tfidf_vectorizer, index, num_recommendations=5)\n",
    "\n",
    "print(f\"Products similar to '{product_name_to_recommend}':\")\n",
    "for product in similar_products:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f4797-7208-487d-94f6-bfe1526feade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc569052-29ec-46e5-bdd8-bac99fcbb49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb3c22-94b4-4e75-8728-440a48efcf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a83fbf-544b-46b9-b059-a258de1b8987",
   "metadata": {},
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7864e40-e1cd-477a-81b5-08e503f7a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering(user_id, df_cleaned, num_recommendations=5):\n",
    "    # Use the trained collaborative filtering model to get top-N recommendations\n",
    "    user_inner_id = algo_item.trainset.to_inner_uid(user_id)\n",
    "    item_neighbors = algo_item.get_neighbors(user_inner_id, k=num_recommendations)\n",
    "    collaborative_recommendations = [algo_item.trainset.to_raw_iid(inner_id) for inner_id in item_neighbors]\n",
    "    \n",
    "    return collaborative_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10927b0d-ff44-4483-8bb6-2337247d69f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid recommended products for user '63295a1878f932890ebe7f33':\n",
      "62ac3cfbe53bef069019d347\n",
      "608e3113c981a3f26588839c\n",
      "5eaaa2b71b450532f9892493\n",
      "617e29c195a1212f22ded67f\n",
      "6065916e334c069770e1319d\n",
      "5f06ae676bc1c84657e1c2b1\n",
      "62b17041d6473c02ea6e5cdc\n",
      "62133796634aa80b0bd5d43e\n",
      "637493e37b92577d060f1c6d\n",
      "6268fe28dde4e73bd13326c9\n"
     ]
    }
   ],
   "source": [
    "def hybrid_recommendation(user_id, df_cleaned, content_index, tfidf_vectorizer, num_recommendations=10):\n",
    "    # Step 1: Get collaborative filtering recommendations\n",
    "    # We'll take the top-N recommendations from collaborative filtering\n",
    "    collaborative_recommendations = collaborative_filtering(user_id, df_cleaned, 10)\n",
    "    \n",
    "    # Step 2: Get content-based recommendations\n",
    "    # We'll take the top-N recommendations from content-based filtering for each item in the collaborative list\n",
    "    content_based_recommendations = set()\n",
    "    for product_id in collaborative_recommendations:\n",
    "        content_recommendations = recommend_products(product_id, df_cleaned, content_index, tfidf_vectorizer, num_recommendations=num_recommendations)\n",
    "        content_based_recommendations.update(content_recommendations)\n",
    "    \n",
    "    # Step 3: Combine both sets of recommendations and limit to the top-N unique items\n",
    "    hybrid_recommendations = list(set(collaborative_recommendations).union(content_based_recommendations))[:num_recommendations]\n",
    "    \n",
    "    return hybrid_recommendations\n",
    "\n",
    "# Example usage\n",
    "user_id_to_recommend = '63295a1878f932890ebe7f33'  # Replace with an actual user_id from your dataset\n",
    "\n",
    "# Assuming you have df_cleaned, index, and tfidf_vectorizer prepared from your previous steps\n",
    "recommended_products_hybrid = hybrid_recommendation(user_id_to_recommend, df_cleaned, index, tfidf_vectorizer, num_recommendations=10)\n",
    "\n",
    "print(f\"Hybrid recommended products for user '{user_id_to_recommend}':\")\n",
    "for product in recommended_products_hybrid:\n",
    "    print(product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acaa1dc-e27b-46ff-bc17-3f9419dec784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ed0fb-1fb3-43de-b003-bcec69ba2d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901152e-5312-4044-8a05-30e7f60c0c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa6056e-ae52-4586-bfc4-aae6da6f6d05",
   "metadata": {},
   "source": [
    "## Deep Learning (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28bf62d4-40a2-41cb-a6b7-b6bfe51ea56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbbafc3f-30bc-44d0-b360-e31ee5948329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a89b5ba2-411b-40fe-9183-a316922d3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with Dask\n",
    "df_dask = dd.read_csv('cartItemsWithRating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa548c2c-f100-45b2-b20e-4a00433c4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "df_dask_cleaned = df_dask.dropna(subset=['customer_id', 'product_id', 'rating'])\n",
    "df_dask_cleaned['customer_id'] = df_dask_cleaned['customer_id'].astype(str)\n",
    "df_dask_cleaned['product_id'] = df_dask_cleaned['product_id'].astype(str)\n",
    "df_dask_cleaned = df_dask_cleaned.drop_duplicates(subset=['customer_id', 'product_id'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67c0b4ed-ed78-49b6-88d9-bb4d0e3f1fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category</th>\n",
       "      <th>_id</th>\n",
       "      <th>status</th>\n",
       "      <th>is_order</th>\n",
       "      <th>is_payed</th>\n",
       "      <th>url_key</th>\n",
       "      <th>sku_from_system</th>\n",
       "      <th>...</th>\n",
       "      <th>store_name</th>\n",
       "      <th>cart_id</th>\n",
       "      <th>added_by</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_url_key</th>\n",
       "      <th>product_description</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>6384488ff60bd378e56c20e4</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>6384488ff60bd378e56c20e3</td>\n",
       "      <td>638447f64e70487901761ce8</td>\n",
       "      <td>638447f64e70487901761ce8</td>\n",
       "      <td>638447f64e70487901761ce8</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Propods P9 Wireless Gaming Headphones Ipx47</td>\n",
       "      <td>632adeebe387a0ccbd419f38</td>\n",
       "      <td>638590305a556431bf362c0a</td>\n",
       "      <td>1000 Ma Bazar</td>\n",
       "      <td>63b850e0eb006452294ea2fc</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>1378_105926346_NP-1027832760</td>\n",
       "      <td>...</td>\n",
       "      <td>Rk Jha Traders</td>\n",
       "      <td>63b850e0eb006452294ea2fb</td>\n",
       "      <td>63b84f58304ae25ff9fa5e91</td>\n",
       "      <td>63b84f58304ae25ff9fa5e91</td>\n",
       "      <td>63b84f58304ae25ff9fa5e91</td>\n",
       "      <td>5e2aa5e3bc8d203bec624d50</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>propods-p9-wireless-gaming-headphones-ipx47</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li data-spm-anchor-id=\"a2a0e.pdp.prod...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Butter Bite Premium 50gm</td>\n",
       "      <td>5e85f0e6e463693ae6691f6f</td>\n",
       "      <td>5e85598cdf1c8d36834577d2</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>5e86acdbe463693ae6692d75</td>\n",
       "      <td>cart</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>butter-bite-premium-50gm</td>\n",
       "      <td>161_butter-bite-premium-50gm</td>\n",
       "      <td>...</td>\n",
       "      <td>Suyash International Pvt Ltd</td>\n",
       "      <td>5e86ac35e463693ae6692d41</td>\n",
       "      <td>5e86abcde463693ae6692d2d</td>\n",
       "      <td>5e86abcde463693ae6692d2d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5e85d9bbf6f6d43a172e2d8d</td>\n",
       "      <td>PriyaGold</td>\n",
       "      <td>butter-bite-premium-50gm</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Ensure moderate butter consumption...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Chocogold Choclate (14gms)</td>\n",
       "      <td>5e862724e463693ae669251c</td>\n",
       "      <td>5e85598cdf1c8d36834577d2</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>5e86b02ae463693ae6692e07</td>\n",
       "      <td>delivered</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>chocogold-choclate-14gms</td>\n",
       "      <td>161_priyagold-chocogold-14gms</td>\n",
       "      <td>...</td>\n",
       "      <td>Suyash International Pvt Ltd</td>\n",
       "      <td>5e86b02ae463693ae6692e06</td>\n",
       "      <td>5e3ba8f268c0ba28de3d5113</td>\n",
       "      <td>5e3ba8f268c0ba28de3d5113</td>\n",
       "      <td>5e3ba8f268c0ba28de3d5113</td>\n",
       "      <td>5e85d9bbf6f6d43a172e2d8d</td>\n",
       "      <td>PriyaGold</td>\n",
       "      <td>chocogold-choclate-14gms</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Brand : Priyagold&lt;/li&gt;\\n\\t&lt;li&gt;Cuis...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Digestive Biscuit 250gm</td>\n",
       "      <td>5e861aede463693ae6692308</td>\n",
       "      <td>5e85598cdf1c8d36834577d2</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>5e86d28ce463693ae6693025</td>\n",
       "      <td>cart</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>digestive-biscuit-250gm</td>\n",
       "      <td>161_digestive-biscuit-250gm</td>\n",
       "      <td>...</td>\n",
       "      <td>Suyash International Pvt Ltd</td>\n",
       "      <td>5e86d28ce463693ae6693024</td>\n",
       "      <td>5e44fb0ed1d01b7bca004b07</td>\n",
       "      <td>5e44fb0ed1d01b7bca004b07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5e85d9bbf6f6d43a172e2d8d</td>\n",
       "      <td>PriyaGold</td>\n",
       "      <td>digestive-biscuit-250gm</td>\n",
       "      <td>&lt;ul&gt;\\n\\t&lt;li&gt;Ensure rich source of fibre in eve...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   product_name                product_id  \\\n",
       "18  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "33  Propods P9 Wireless Gaming Headphones Ipx47  632adeebe387a0ccbd419f38   \n",
       "66                     Butter Bite Premium 50gm  5e85f0e6e463693ae6691f6f   \n",
       "72                   Chocogold Choclate (14gms)  5e862724e463693ae669251c   \n",
       "92                      Digestive Biscuit 250gm  5e861aede463693ae6692308   \n",
       "\n",
       "                 category_id       category                       _id  \\\n",
       "18  638590305a556431bf362c0a  1000 Ma Bazar  6384488ff60bd378e56c20e4   \n",
       "33  638590305a556431bf362c0a  1000 Ma Bazar  63b850e0eb006452294ea2fc   \n",
       "66  5e85598cdf1c8d36834577d2      Groceries  5e86acdbe463693ae6692d75   \n",
       "72  5e85598cdf1c8d36834577d2      Groceries  5e86b02ae463693ae6692e07   \n",
       "92  5e85598cdf1c8d36834577d2      Groceries  5e86d28ce463693ae6693025   \n",
       "\n",
       "       status  is_order  is_payed  \\\n",
       "18    removed     False     False   \n",
       "33    removed     False     False   \n",
       "66       cart     False     False   \n",
       "72  delivered     False     False   \n",
       "92       cart     False     False   \n",
       "\n",
       "                                        url_key  \\\n",
       "18  propods-p9-wireless-gaming-headphones-ipx47   \n",
       "33  propods-p9-wireless-gaming-headphones-ipx47   \n",
       "66                     butter-bite-premium-50gm   \n",
       "72                     chocogold-choclate-14gms   \n",
       "92                      digestive-biscuit-250gm   \n",
       "\n",
       "                  sku_from_system  ...                    store_name  \\\n",
       "18   1378_105926346_NP-1027832760  ...                Rk Jha Traders   \n",
       "33   1378_105926346_NP-1027832760  ...                Rk Jha Traders   \n",
       "66   161_butter-bite-premium-50gm  ...  Suyash International Pvt Ltd   \n",
       "72  161_priyagold-chocogold-14gms  ...  Suyash International Pvt Ltd   \n",
       "92    161_digestive-biscuit-250gm  ...  Suyash International Pvt Ltd   \n",
       "\n",
       "                     cart_id                  added_by  \\\n",
       "18  6384488ff60bd378e56c20e3  638447f64e70487901761ce8   \n",
       "33  63b850e0eb006452294ea2fb  63b84f58304ae25ff9fa5e91   \n",
       "66  5e86ac35e463693ae6692d41  5e86abcde463693ae6692d2d   \n",
       "72  5e86b02ae463693ae6692e06  5e3ba8f268c0ba28de3d5113   \n",
       "92  5e86d28ce463693ae6693024  5e44fb0ed1d01b7bca004b07   \n",
       "\n",
       "                 customer_id                   user_id  \\\n",
       "18  638447f64e70487901761ce8  638447f64e70487901761ce8   \n",
       "33  63b84f58304ae25ff9fa5e91  63b84f58304ae25ff9fa5e91   \n",
       "66  5e86abcde463693ae6692d2d                       NaN   \n",
       "72  5e3ba8f268c0ba28de3d5113  5e3ba8f268c0ba28de3d5113   \n",
       "92  5e44fb0ed1d01b7bca004b07                       NaN   \n",
       "\n",
       "                    brand_id           brand  \\\n",
       "18  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "33  5e2aa5e3bc8d203bec624d50  Not Applicable   \n",
       "66  5e85d9bbf6f6d43a172e2d8d       PriyaGold   \n",
       "72  5e85d9bbf6f6d43a172e2d8d       PriyaGold   \n",
       "92  5e85d9bbf6f6d43a172e2d8d       PriyaGold   \n",
       "\n",
       "                                product_url_key  \\\n",
       "18  propods-p9-wireless-gaming-headphones-ipx47   \n",
       "33  propods-p9-wireless-gaming-headphones-ipx47   \n",
       "66                     butter-bite-premium-50gm   \n",
       "72                     chocogold-choclate-14gms   \n",
       "92                      digestive-biscuit-250gm   \n",
       "\n",
       "                                  product_description rating  \n",
       "18  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    1.5  \n",
       "33  <ul>\\n\\t<li data-spm-anchor-id=\"a2a0e.pdp.prod...    2.5  \n",
       "66  <ul>\\n\\t<li>Ensure moderate butter consumption...    1.5  \n",
       "72  <ul>\\n\\t<li>Brand : Priyagold</li>\\n\\t<li>Cuis...    1.0  \n",
       "92  <ul>\\n\\t<li>Ensure rich source of fibre in eve...    3.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dask_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e654751-6d0d-47e5-be23-e08f551b2c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.1 GiB for an array with shape (56714, 28631) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the user-item matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m user_item_matrix \u001b[38;5;241m=\u001b[39m df_dask_cleaned\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9339\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[1;34m(self, columns, index, values)\u001b[0m\n\u001b[0;32m   9332\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9333\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\n\u001b[0;32m   9335\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m   9336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9337\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[1;32m-> 9339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, values\u001b[38;5;241m=\u001b[39mvalues)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:570\u001b[0m, in \u001b[0;36mpivot\u001b[1;34m(data, columns, index, values)\u001b[0m\n\u001b[0;32m    566\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m result \u001b[38;5;241m=\u001b[39m indexed\u001b[38;5;241m.\u001b[39munstack(columns_listlike)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    571\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    572\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[0;32m    573\u001b[0m ]\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4615\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[1;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[0;32m   4570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4571\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[0;32m   4572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4611\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[0;32m   4612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4613\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 4615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstack(\u001b[38;5;28mself\u001b[39m, level, fill_value, sort)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:520\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m    517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m _Unstacker(\n\u001b[0;32m    518\u001b[0m     obj\u001b[38;5;241m.\u001b[39mindex, level\u001b[38;5;241m=\u001b[39mlevel, constructor\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_expanddim, sort\u001b[38;5;241m=\u001b[39msort\n\u001b[0;32m    519\u001b[0m )\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[0;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:238\u001b[0m, in \u001b[0;36m_Unstacker.get_result\u001b[1;34m(self, values, value_columns, fill_value)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust pass column labels for multi-column data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m values, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_new_values(values, fill_value)\n\u001b[0;32m    239\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_new_columns(value_columns)\n\u001b[0;32m    240\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_index\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:289\u001b[0m, in \u001b[0;36m_Unstacker.get_new_values\u001b[1;34m(self, values, fill_value)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m         dtype, fill_value \u001b[38;5;241m=\u001b[39m maybe_promote(dtype, fill_value)\n\u001b[1;32m--> 289\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(result_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    290\u001b[0m         new_values\u001b[38;5;241m.\u001b[39mfill(fill_value)\n\u001b[0;32m    292\u001b[0m name \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.1 GiB for an array with shape (56714, 28631) and data type float64"
     ]
    }
   ],
   "source": [
    "# Compute the user-item matrix\n",
    "user_item_matrix = df_dask_cleaned.compute().pivot(index='customer_id', columns='product_id', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea5ba5cc-be4c-4d20-8f2c-060734249e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5176\\2593955251.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['customer_id'] = df_cleaned['customer_id'].astype(str)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5176\\2593955251.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['product_id'] = df_cleaned['product_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('cartItemsWithRating.csv')\n",
    "\n",
    "# Remove missing values and duplicates\n",
    "df_cleaned = df.dropna(subset=['customer_id', 'product_id', 'rating'])\n",
    "df_cleaned['customer_id'] = df_cleaned['customer_id'].astype(str)\n",
    "df_cleaned['product_id'] = df_cleaned['product_id'].astype(str)\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['customer_id', 'product_id'], keep='last')\n",
    "\n",
    "sample_df = df_cleaned.head(100)\n",
    "# Create a sparse matrix\n",
    "user_ids = sample_df['customer_id'].astype('category').cat.codes\n",
    "product_ids = sample_df['product_id'].astype('category').cat.codes\n",
    "ratings = sample_df['rating']\n",
    "\n",
    "sparse_matrix = sp.coo_matrix((ratings, (user_ids, product_ids)))\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "user_item_matrix = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac3e9756-9d25-4025-9322-5cca1b4ee5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2    3   4   5    6    7   8   9   ...  19  20  21  22  23  24  \\\n",
       "0     0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "1   3.0   0   0    0   0   0    0  1.0   0   0  ...   0   0   0   0   0   0   \n",
       "2     0   0   0    0   0   0  3.0    0   0   0  ...   0   0   0   0   0   0   \n",
       "3     0   0   0  2.0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "4   1.0   0   0  2.0   0   0    0  1.5   0   0  ...   0   0   0   0   0   0   \n",
       "..  ...  ..  ..  ...  ..  ..  ...  ...  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
       "71    0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "72    0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "73    0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "74    0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "75    0   0   0    0   0   0    0    0   0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "    25   26  27  28  \n",
       "0    0    0   0   0  \n",
       "1    0    0   0   0  \n",
       "2    0    0   0   0  \n",
       "3    0    0   0   0  \n",
       "4    0    0   0   0  \n",
       "..  ..  ...  ..  ..  \n",
       "71   0  1.0   0   0  \n",
       "72   0  3.0   0   0  \n",
       "73   0  3.0   0   0  \n",
       "74   0  1.5   0   0  \n",
       "75   0  3.0   0   0  \n",
       "\n",
       "[76 rows x 29 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82043f07-f79a-4827-b76c-a7e5d7a5b3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
